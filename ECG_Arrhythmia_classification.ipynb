{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/med-adam-alimi/Development-of-a-Deep-Learning-model-for-ECG-based-arrhythmia-classification/blob/main/ECG_Arrhythmia_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8SFjY_hJh_M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1pM5hcyKenr"
      },
      "outputs": [],
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwOgzed1Kee8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdO1ocUuKece"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
        "test_data = pd.read_csv(\"mitbih_test.csv\", header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xNr_JG3F_ha"
      },
      "source": [
        "###Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVKnp_EnKeaG"
      },
      "outputs": [],
      "source": [
        "# Show the shape and the first few rows of the train_data\n",
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CyItwyh45Uu"
      },
      "outputs": [],
      "source": [
        "train_data.iloc[:, -1].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSdpniD9KeUk"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mpF9Q7XKeXc"
      },
      "outputs": [],
      "source": [
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "974AUikxHWXN"
      },
      "outputs": [],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLawm3iYGtJv"
      },
      "outputs": [],
      "source": [
        "print(train_data.isnull().sum())\n",
        "print(test_data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_SsL_s0KeSG"
      },
      "outputs": [],
      "source": [
        "train_data.iloc[:, -1].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IWcS0ZkH7rI"
      },
      "outputs": [],
      "source": [
        "train_data.iloc[:, -1].value_counts().plot(kind='bar', title='Class Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyrrAxiL02Yc"
      },
      "source": [
        "###Seperate features and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4Tx_KrO0KJ9"
      },
      "outputs": [],
      "source": [
        "X_train=train_data.iloc[:,:-1].values\n",
        "y_train=train_data.iloc[:,-1].values\n",
        "\n",
        "X_test=test_data.iloc[:,:-1].values\n",
        "y_test=test_data.iloc[:,-1].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YApuP4L2VAL"
      },
      "source": [
        "###Normalize the ECG signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rngA4fa51N4X"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1acvOlHv2m0R"
      },
      "source": [
        "###visualize Random Heartbeats from the train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsQzmHKX2fEt"
      },
      "outputs": [],
      "source": [
        "#Shuffle the train_data\n",
        "from sklearn.utils import shuffle\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "plt.figure(figsize=(10, 7))\n",
        "import random\n",
        "random_indices = random.sample(range(len(X_train)), 5)\n",
        "for i, idx in enumerate(random_indices):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.plot(X_train[idx])\n",
        "    plt.title(f\"Label: {int(y_train[idx])}\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "#plt.suptitle(\"Random Heartbeats from Training Data\", fontsize=16)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shaQRsft4Yn3"
      },
      "source": [
        "###Convert to a PyTorch Dataset\n",
        "\n",
        "*   Élément de liste\n",
        "*   Élément de liste\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9r1p7A_2tom"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_32_STy4eYK"
      },
      "outputs": [],
      "source": [
        "train_dataset = ECGDataset(X_train, y_train)\n",
        "test_dataset = ECGDataset(X_test, y_test)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-z4aw5b6v6S"
      },
      "source": [
        ":###Generate Wavelet Features(for Dual Branch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9zKXcIh6gq9"
      },
      "outputs": [],
      "source": [
        "!pip install PyWavelets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEAh0Xua6sbb"
      },
      "outputs": [],
      "source": [
        "import pywt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEQA2E_X7DP-"
      },
      "outputs": [],
      "source": [
        "# --- Wavelet Feature Extraction ---\n",
        "def compute_wavelet(signal):\n",
        "    coeffs, _ = pywt.cwt(signal, scales=np.arange(1, 31), wavelet='morl')\n",
        "    return np.abs(coeffs).mean(axis=0)  # simple summary\n",
        "\n",
        "# Apply to all training and testing data\n",
        "wavelet_train = np.array([compute_wavelet(x) for x in X_train])\n",
        "wavelet_test = np.array([compute_wavelet(x) for x in X_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dNZO0Db_d-3"
      },
      "source": [
        "###Define Dual_Branch Transformer model\n",
        "####Each branch: 1D Transformer for raw or wavelet input\n",
        "####Then: Concatenate → FC Layer → Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR3fg8U9_WDk"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ECGTransformer(nn.Module):\n",
        "    def __init__(self, input_len):\n",
        "        super(ECGTransformer, self).__init__()\n",
        "\n",
        "        self.raw_branch = nn.Sequential(\n",
        "            nn.Linear(input_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(128)\n",
        "        )\n",
        "\n",
        "        self.freq_branch = nn.Sequential(\n",
        "            nn.Linear(input_len, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(128)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 5)  # 5 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, raw, freq):\n",
        "        raw_out = self.raw_branch(raw)\n",
        "        freq_out = self.freq_branch(freq)\n",
        "        combined = torch.cat((raw_out, freq_out), dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "model=ECGTransformer(input_len=X_train.shape[1]).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b87fGRJAglV"
      },
      "source": [
        "###Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2KKfbD4Ac_X"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqaOnkFLB68b"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy (a classification metric)\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
        "    acc = (correct / len(y_pred)) * 100\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbIQaOj7A52f"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "epochs=3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i, (raw, labels) in enumerate(train_dataloader):\n",
        "        raw, labels = raw.to(device), labels.to(device)\n",
        "        freq = torch.tensor([compute_wavelet(x.cpu().numpy()) for x in raw]).to(device)\n",
        "\n",
        "        y_preds = model(raw, freq)\n",
        "        loss = loss_fn(y_preds, labels)\n",
        "        acc=accuracy_fn(labels,y_preds.argmax(dim=1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ-m8StKKegj"
      },
      "source": [
        "##As we can see our model is overfitting\n",
        "## modifications to prevent overfitting:\n",
        "####Data augmentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq8MECFwDjFg"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation\n",
        "import torch.nn.functional as F\n",
        "class ECGAugmentation:\n",
        "    def __call__(self, x):\n",
        "        # Add random noise\n",
        "        x += torch.randn_like(x) * 0.01\n",
        "        # Random time warping\n",
        "        # Fix: Use a fixed scaling factor for all samples in the batch\n",
        "        scale_factor = random.uniform(0.9, 1.1)  # Generate scale factor outside loop\n",
        "        if random.random() > 0.5:\n",
        "            x = F.interpolate(x.unsqueeze(0).unsqueeze(0),\n",
        "                            scale_factor=scale_factor, # Use fixed factor\n",
        "                            mode='linear').squeeze()\n",
        "        # Pad or truncate to ensure consistent length\n",
        "        target_len = 187  # Choose the desired length\n",
        "        x = F.pad(x, (0, target_len - len(x))) if len(x) < target_len else x[:target_len]\n",
        "        return x\n",
        "\n",
        "# Create augmented dataset\n",
        "class AugmentedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, augment):\n",
        "        self.dataset = dataset\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.dataset[idx]\n",
        "        if self.augment:\n",
        "            x = self.augment(x)\n",
        "        return x, y\n",
        "\n",
        "augment = ECGAugmentation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q8HRArtK7lL"
      },
      "outputs": [],
      "source": [
        "train_dataset = AugmentedDataset(train_dataset, augment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4yLXJQkLa9D"
      },
      "outputs": [],
      "source": [
        "test_dataset=AugmentedDataset(test_dataset, augment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8bpBGroMT9C"
      },
      "source": [
        "####Handle class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PmGs-MxL8hK"
      },
      "outputs": [],
      "source": [
        "# Handle class imbalance\n",
        "# Convert y_train to integers before using np.bincount\n",
        "y_train_int = y_train.astype(int)\n",
        "class_counts = np.bincount(y_train_int)\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "class_weights = class_weights.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtSRjDfZMYeG"
      },
      "outputs": [],
      "source": [
        "# Model Architecture (Dual-Branch Transformer)\n",
        "class ECGTransformerV1(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, dropout_rate=0.5):\n",
        "        super(ECGTransformerV1, self).__init__()\n",
        "\n",
        "          # Add gradient clipping in your training loop\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Temporal branch\n",
        "        self.temporal_proj = nn.Linear(input_dim, 64)\n",
        "        self.temporal_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=64, nhead=8, dim_feedforward=256),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "        # Frequency branch (assuming wavelet features)\n",
        "        self.freq_proj = nn.Linear(input_dim, 64)\n",
        "        self.freq_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=64, nhead=8, dim_feedforward=256),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "        # Combine branches\n",
        "        self.combine = nn.Linear(128, 64)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(64, num_classes)\n",
        "\n",
        "        # Batch normalization\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Temporal branch\n",
        "        x_temp = self.temporal_proj(x)\n",
        "        x_temp = self.bn1(x_temp.transpose(1, 2)).transpose(1, 2)\n",
        "        x_temp = self.temporal_transformer(x_temp)\n",
        "        x_temp = x_temp.mean(dim=1)\n",
        "\n",
        "        # Frequency branch (using same input for demo - replace with wavelet features)\n",
        "        x_freq = self.freq_proj(x)\n",
        "        x_freq = self.bn2(x_freq.transpose(1, 2)).transpose(1, 2)\n",
        "        x_freq = self.freq_transformer(x_freq)\n",
        "        x_freq = x_freq.mean(dim=1)\n",
        "\n",
        "        # Combine branches\n",
        "        x = torch.cat([x_temp, x_freq], dim=1)\n",
        "        x = self.combine(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "model_1=ECGTransformerV1(input_dim=X_train.shape[1],num_classes=5).to(device)\n",
        "model_1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mues4I7_NQoN"
      },
      "outputs": [],
      "source": [
        "# Loss function with class weights\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer with weight decay (L2 regularization)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R39JP0qyOkK6"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7XE_Hs5PBsN"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "epochs=3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    #loss = 0\n",
        "    #correct = 0\n",
        "    #total = 0\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Calculate frequency features\n",
        "        freq = torch.tensor([compute_wavelet(x.cpu().numpy()) for x in inputs]).to(device)\n",
        "\n",
        "        # Forward pass with both raw and frequency data\n",
        "        outputs = model(inputs, freq)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        acc=accuracy_fn(labels,outputs.argmax(dim=1))\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1} | Loss: {loss:.4f} , Accuracy: {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU_wY8_HVO7p"
      },
      "source": [
        "###As we see our model is still overfitting , we need other modifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6HI9alZQ8Jc"
      },
      "source": [
        "### Modifications to prevent overfitting part 2: We will:\n",
        "#####apply cross-validation on the training data.\n",
        "#####Make advanced data augmentation\n",
        "#####Add Early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DALa8sH9jDCv"
      },
      "source": [
        "###Apply cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkSe_3odjCAR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "X = train_data.iloc[:,:-1].values\n",
        "y = train_data.iloc[:,-1].values\n",
        "\n",
        "k_folds = 5\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    print(f'\\nFOLD {fold + 1}')\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlbGeJCQR9kz"
      },
      "outputs": [],
      "source": [
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MK2Y79DRXwf"
      },
      "outputs": [],
      "source": [
        "train_dataset = ECGDataset(X_train, y_train)\n",
        "val_dataset = ECGDataset(X_val, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4rl9CAGSl4l"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By2x0Ws9TZ1t"
      },
      "outputs": [],
      "source": [
        "# --- Wavelet Feature Extraction ---\n",
        "def compute_wavelet(signal):\n",
        "    coeffs, _ = pywt.cwt(signal, scales=np.arange(1, 31), wavelet='morl')\n",
        "    return np.abs(coeffs).mean(axis=0)\n",
        "\n",
        "# Apply to all training and validation data\n",
        "wavelet_train = np.array([compute_wavelet(x) for x in X_train])\n",
        "wavelet_val = np.array([compute_wavelet(x) for x in X_val])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkJsjqk4T_ap"
      },
      "source": [
        "###Advanced Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_tb_zYuT5ie"
      },
      "outputs": [],
      "source": [
        "#class ECGAugmentation:\n",
        "#    def __call__(self, x):\n",
        "        # Add random noise\n",
        "#        x += torch.randn_like(x) * 0.01\n",
        "        # Random time warping\n",
        "#        if random.random() > 0.5:\n",
        "#            x = F.interpolate(x.unsqueeze(0).unsqueeze(0),\n",
        "#                            scale_factor=random.uniform(0.9, 1.1),\n",
        "#                            mode='linear').squeeze()\n",
        "#        return x\n",
        "class ECGAugmentation:\n",
        "    def __call__(self, x):\n",
        "        # Existing augmentations\n",
        "        x += torch.randn_like(x) * 0.01\n",
        "\n",
        "        # New augmentations:\n",
        "        # Random scaling\n",
        "        scale = random.uniform(0.8, 1.2)\n",
        "        x = x * scale\n",
        "\n",
        "        # Random channel dropout\n",
        "        if random.random() < 0.1:\n",
        "            chan = random.randint(0, x.size(0)-1)\n",
        "            x[chan] = 0\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create augmented dataset\n",
        "class AugmentedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, augment):\n",
        "        self.dataset = dataset\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.dataset[idx]\n",
        "        if self.augment:\n",
        "            x = self.augment(x)\n",
        "        return x, y\n",
        "\n",
        "augment = ECGAugmentation()\n",
        "train_dataset = AugmentedDataset(train_dataset, augment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIV2D2eyUQIQ"
      },
      "source": [
        "###Handle class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQta8Wc3UNVW"
      },
      "outputs": [],
      "source": [
        "class_counts = np.bincount(y_train.astype(int))\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "class_weights = class_weights.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgwAvcm2Tn_Z"
      },
      "outputs": [],
      "source": [
        "# Model Architecture (Dual-Branch Transformer)\n",
        "class ECGTransformerV3(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, dropout_rate=0.5):\n",
        "        super(ECGTransformerV3, self).__init__()\n",
        "\n",
        "        # Temporal branch\n",
        "        self.temporal_proj = nn.Linear(input_dim, 64)\n",
        "        self.temporal_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=64, nhead=8, dim_feedforward=256),\n",
        "            num_layers=3\n",
        "        )\n",
        "\n",
        "        # Frequency branch (assuming wavelet features)\n",
        "        self.freq_proj = nn.Linear(input_dim, 64)\n",
        "        self.freq_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=64, nhead=8, dim_feedforward=256),\n",
        "            num_layers=3\n",
        "        )\n",
        "\n",
        "        # Combine branches\n",
        "        self.combine = nn.Linear(128, 64)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(64, num_classes)\n",
        "\n",
        "        # Batch normalization\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Temporal branch\n",
        "        x_temp = self.temporal_proj(x)\n",
        "        x_temp = self.bn1(x_temp.transpose(1, 2)).transpose(1, 2)\n",
        "        x_temp = self.temporal_transformer(x_temp)\n",
        "        x_temp = x_temp.mean(dim=1)\n",
        "\n",
        "        # Frequency branch (using same input for demo - replace with wavelet features)\n",
        "        x_freq = self.freq_proj(x)\n",
        "        x_freq = self.bn2(x_freq.transpose(1, 2)).transpose(1, 2)\n",
        "        x_freq = self.freq_transformer(x_freq)\n",
        "        x_freq = x_freq.mean(dim=1)\n",
        "\n",
        "        # Combine branches\n",
        "        x = torch.cat([x_temp, x_freq], dim=1)\n",
        "        x = self.combine(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model_3 = ECGTransformerV3(input_dim=X_train.shape[1], num_classes=5).to(device)\n",
        "model_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNd82mViU5-3"
      },
      "outputs": [],
      "source": [
        "# Loss function with class weights\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer with weight decay (L2 regularization)\n",
        "optimizer = torch.optim.Adam(model_3.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69UawU-vjBYw"
      },
      "source": [
        "###Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6vFnYhAVI-F"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "epochs=3\n",
        "for epoch in range(epochs):\n",
        "  model_3.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for inputs, labels in train_dataloader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "    outputs = model_3(inputs.unsqueeze(1))\n",
        "    loss = loss_fn(outputs, labels)\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "  model_3.eval()\n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model_3(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      val_total += labels.size(0)\n",
        "      val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "      # Calculate metrics\n",
        "  train_loss = running_loss / len(train_dataloader)\n",
        "  val_loss = val_loss / len(val_dataloader)\n",
        "  train_acc = 100. * correct / total\n",
        "  val_acc = 100. * val_correct / val_total\n",
        "\n",
        "  # Update learning rate\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "        # Store metrics\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  val_accs.append(val_acc)\n",
        "\n",
        "  print(f'Epoch: {epoch} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.2f}% | '\n",
        "              f'Val Loss: {val_loss:.5f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model_3.state_dict(), 'best_model.pth')\n",
        "    patience_counter = 0\n",
        "  else:\n",
        "    patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "      print(\"Early stopping triggered\")\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-XqjYysnJlw"
      },
      "source": [
        "###As observed in the training results, there is some fluctuation in the validation accuracy across epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwJcmZR6UFi5"
      },
      "source": [
        "####Minimize fluctuation in the val accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zL3S8j-freq"
      },
      "outputs": [],
      "source": [
        "# Loss function with class weights\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer with weight decay (L2 regularization)\n",
        "optimizer = torch.optim.Adam(model_3.parameters(), lr=0.001, weight_decay=1e-4,eps=1e-8)\n",
        "\n",
        "# Learning rate scheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1,patience=3, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dC7LTVL0jQLv"
      },
      "source": [
        "###Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbHF0p_rY0cN"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "accumulation_steps = 2\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "epochs=6\n",
        "for epoch in range(epochs):\n",
        "  model_3.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for i,(inputs, labels) in enumerate(train_dataloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "    with torch.cuda.amp.autocast():\n",
        "      outputs = model_3(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)/accumulation_steps\n",
        "\n",
        "    # Backward pass and optimizer\n",
        "    loss.backward()\n",
        "    if (i + 1) % accumulation_steps == 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model_3.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "  model_3.eval()\n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model_3(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      val_total += labels.size(0)\n",
        "      val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "      # Calculate metrics\n",
        "  train_loss = running_loss / len(train_dataloader)\n",
        "  val_loss = val_loss / len(val_dataloader)\n",
        "  train_acc = 100. * correct / total\n",
        "  val_acc = 100. * val_correct / val_total\n",
        "\n",
        "  # Update learning rate\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "        # Store metrics\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  val_accs.append(val_acc)\n",
        "\n",
        "  print(f'Epoch: {epoch+1} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.2f}% | '\n",
        "              f'Val Loss: {val_loss:.5f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model_3.state_dict(), 'best_model.pth')\n",
        "    patience_counter = 0\n",
        "  else:\n",
        "    patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "      print(\"Early stopping triggered\")\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mmtB_ASjXju"
      },
      "source": [
        "###Training and testing loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHwcWT3un8uS"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "accumulation_steps = 2\n",
        "train_losses, val_losses ,test_losses= [], [],[]\n",
        "train_accs, val_accs,test_accs = [], [],[]\n",
        "epochs=10\n",
        "for epoch in range(epochs):\n",
        "  model_3.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for i,(inputs, labels) in enumerate(train_dataloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "    with torch.cuda.amp.autocast():\n",
        "      outputs = model_3(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)/accumulation_steps\n",
        "\n",
        "    # Backward pass and optimizer\n",
        "    loss.backward()\n",
        "    if (i + 1) % accumulation_steps == 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model_3.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "  model_3.eval()\n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model_3(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      val_total += labels.size(0)\n",
        "      val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Testing\n",
        "  model_3.eval()\n",
        "  test_loss = 0.0\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model_3(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      test_total += labels.size(0)\n",
        "      test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "\n",
        "      # Calculate metrics\n",
        "  train_loss = running_loss / len(train_dataloader)\n",
        "  val_loss = val_loss / len(val_dataloader)\n",
        "  test_loss=test_loss/len(test_dataloader)\n",
        "  train_acc = 100. * correct / total\n",
        "  val_acc = 100. * val_correct / val_total\n",
        "  test_acc=100.*test_correct/test_total\n",
        "\n",
        "  # Update learning rate\n",
        "  scheduler.step(test_loss)\n",
        "\n",
        "        # Store metrics\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "  test_losses.append(test_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  val_accs.append(val_acc)\n",
        "  test_accs.append(test_acc)\n",
        "\n",
        "  print(f'Epoch: {epoch+1} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.2f}% | '\n",
        "              f'Val Loss: {val_loss:.5f}, Val Acc: {val_acc:.2f}% |'\n",
        "              f'Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "        # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model_3.state_dict(), 'best_model.pth')\n",
        "    patience_counter = 0\n",
        "  else:\n",
        "    patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "      print(\"Early stopping triggered\")\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drUOgyvcn8qy"
      },
      "outputs": [],
      "source": [
        "model_3_results = pd.DataFrame([{\n",
        "    'model_name': model_3.__class__.__name__,\n",
        "    'model_loss': test_loss,\n",
        "    'model_acc': test_acc\n",
        "}])\n",
        "\n",
        "model_3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvjlEQCZ1p8E"
      },
      "source": [
        "##Make and evaluate random prediction with our best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yERjaWP3n8og"
      },
      "outputs": [],
      "source": [
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "    pred_probs = []\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for sample in data:\n",
        "            # Prepare sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n",
        "            # Reshape the sample to have 3 dimensions for the model input (unsqueeze on dim=1)\n",
        "            sample = sample.unsqueeze(1)\n",
        "            # Forward pass (model outputs raw logit)\n",
        "            pred_logit = model(sample)\n",
        "\n",
        "            # Get prediction probability (logit -> prediction probability)\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
        "\n",
        "            # Get pred_prob off GPU for further calculations\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    # Stack the pred_probs to turn list into a tensor\n",
        "    return torch.stack(pred_probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6IHr0-Cn8ls"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "#random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "# Get a list of random row indices\n",
        "random_indices = random.sample(range(len(test_data)), k=9)\n",
        "\n",
        "# Use the indices to access data and labels\n",
        "for idx in random_indices:\n",
        "    # Convert sample to PyTorch tensor\n",
        "    sample = torch.tensor(test_data.iloc[idx, :-1].values, dtype=torch.float32)\n",
        "    label = test_data.iloc[idx, -1]\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# View the first test sample shape and label\n",
        "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqoi_fstn8jG"
      },
      "outputs": [],
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs= make_predictions(model=model_3,\n",
        "                             data=test_samples)\n",
        "\n",
        "# View first 5 prediction probabilities list\n",
        "pred_probs[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8SB2RcJn8gY"
      },
      "outputs": [],
      "source": [
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTq_RUWDn8d4"
      },
      "outputs": [],
      "source": [
        "# Are our predictions in the same form as our test labels?\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "print(f'test labels :{test_labels_tensor}')\n",
        "print(f'pred calsses:{pred_classes}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoP2G6lyn8YZ"
      },
      "outputs": [],
      "source": [
        "#Now let's visualize:\n",
        "# Plot predictions\n",
        "plt.figure(figsize=(9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create a subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  plt.plot(sample.squeeze())\n",
        "\n",
        "  # Find the prediction label (in text form, e.g. \"0\")\n",
        "  pred_label = pred_classes[i]\n",
        "\n",
        "  # Get the truth label (in text form, e.g. \"2\")\n",
        "  truth_label = test_labels[i]\n",
        "\n",
        "  # Create the title text of the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # Check for equality and change title colour accordingly\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
        "  plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIieh5Sg7e5a"
      },
      "source": [
        "###Making a confusion matrix for further prediction evaluation :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzixTCRUn8V7"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "y_preds=[]\n",
        "model_3.eval()\n",
        "with torch.no_grad():\n",
        "  for input, label in tqdm(test_dataloader,desc='Making predictions'):\n",
        "    input, label = input.to(device), label.to(device)\n",
        "    y_pred = model_3(input.unsqueeze(1))\n",
        "    #Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fIHyWWYn8TN"
      },
      "outputs": [],
      "source": [
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
        "except:\n",
        "    !pip install -q torchmetrics -U mlxtend\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaFJ1tDzn8Qk"
      },
      "outputs": [],
      "source": [
        "# Import mlxtend upgraded version\n",
        "import mlxtend\n",
        "print(mlxtend.__version__)\n",
        "assert int(mlxtend.__version__.split(\".\")[1]) >= 19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--g1YUqBn8N_"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# 'y_test' contains the target labels\n",
        "confmat = ConfusionMatrix(num_classes=5, task='multiclass')\n",
        "confmat_tensor = confmat(preds=y_pred_tensor.argmax(dim=1),\n",
        "                         target=torch.tensor(y_test, dtype=torch.int64))  # Convert y_test to a PyTorch tensor\n",
        "\n",
        "# Define class names\n",
        "class_names = ['0', '1', '2', '3', '4']\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(),\n",
        "    class_names=class_names,\n",
        "    figsize=(10, 7)\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d202St4n8Lc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import torch\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    Displays the confusion matrix with or without normalization.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "y_pred_labels = y_pred_tensor.argmax(dim=1).cpu().numpy()\n",
        "y_true_labels = torch.tensor(y_test).cpu().numpy()\n",
        "\n",
        "# --- Compute the confusion matrix using scikit-learn ---\n",
        "cnf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plot_confusion_matrix(cnf_matrix,\n",
        "                      classes= ['0', '1', '2', '3', '4'],\n",
        "                      normalize=True,\n",
        "                      title='Normalized Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGoTJ6Atn8Ix"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_labels, y_pred_labels, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IK3SzEyQ2ks"
      },
      "source": [
        "###Interpretation of the results of the confusion matrix and classification report:   \n",
        "####* for class 0: Excellent : we have high precision and recall\n",
        "####* for class 1: Low precision: there's many false positives , but decent recall\n",
        "####* for class 2: Very good:well balanced\n",
        "####* for class 3:similar to class 1: recall is good but precision needs work\n",
        "####* for class 4: Ecellent: well handled\n",
        "\n",
        "###Overall Metrics:\n",
        "####Accuracy: 95.29% --strong overall\n",
        "####Macro avg F1: 0.8082 --Shows that class imbalance is affecting smaller classes\n",
        "####Weighted avg F1: 0.9560-- reflects strong performance due to dominance of class 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG1btYDNS8Z8"
      },
      "source": [
        "##Save and load our best performing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUBJzJftn8Dt"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True,exist_ok=True )\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"ECG_arrhythmia_classification_model_3.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_3.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
        "           f=MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeeDqWJ2cMS9"
      },
      "source": [
        "#####Now we've got a saved model state_dict() we can load it back in using a combination of load_state_dict() and torch.load().\n",
        "\n",
        "#####Since we're using load_state_dict(), we'll need to create a new instance of ECGTransformerV3() with the same input parameters as our saved model state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXDVFy7GcL_7"
      },
      "outputs": [],
      "source": [
        "# Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n",
        "#Provide input_dim and num_classes while creating an instance of ECGTransformerV3\n",
        "loaded_model_3 = ECGTransformerV3(input_dim=X_train.shape[1],num_classes=5)\n",
        "\n",
        "# Load in the saved state_dict()\n",
        "loaded_model_3.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# Send model to GPU\n",
        "loaded_model_3= loaded_model_3.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFDly70bgFEZ"
      },
      "outputs": [],
      "source": [
        "# iterate through the keys and compare individual tensors using torch.equal()\n",
        "all_equal = True\n",
        "for key in loaded_model_3.state_dict():\n",
        "    if not torch.equal(loaded_model_3.state_dict()[key], loaded_model_3.state_dict()[key]):\n",
        "        all_equal = False\n",
        "        break\n",
        "\n",
        "print(all_equal)  # This will print True if all tensors are equal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCK0fIfwhYhk"
      },
      "source": [
        "####Now that we've got a loaded model we can evaluate it to make sure its parameters work similarly to model_3 prior to saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYB6oqN5dzzU"
      },
      "outputs": [],
      "source": [
        "for epoch in range(10):\n",
        "  loaded_model_3.eval()\n",
        "  loaded_test_loss = 0.0\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = loaded_model_3(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      loaded_test_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      test_total += labels.size(0)\n",
        "      test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "\n",
        "      # Calculate metrics\n",
        "  loaded_test_loss=loaded_test_loss/len(test_dataloader)\n",
        "  loaded_test_acc=100.*test_correct/test_total\n",
        "\n",
        "  # Update learning rate\n",
        "  scheduler.step(test_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12F0mNEwgtHd"
      },
      "outputs": [],
      "source": [
        "loaded_model_3_results = pd.DataFrame([{\n",
        "    'model_name': loaded_model_3.__class__.__name__,\n",
        "    'model_loss': loaded_test_loss,\n",
        "    'model_acc': loaded_test_acc\n",
        "}])\n",
        "loaded_model_3_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSipLvwZhFpy"
      },
      "outputs": [],
      "source": [
        "model_3_results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}