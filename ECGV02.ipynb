{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/med-adam-alimi/Development-of-a-Deep-Learning-model-for-ECG-based-arrhythmia-classification/blob/main/ECGV02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TTmPOZ3aMMF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "JJMaUer5aOtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "QiT5adroa3VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
        "test_data = pd.read_csv(\"mitbih_test.csv\", header=None)"
      ],
      "metadata": {
        "id": "IAXTz9CUbDlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "4fsRq_uwuAaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "P3l5k96Et7Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.iloc[:, -1].value_counts().sort_index()"
      ],
      "metadata": {
        "id": "bUnxpP9MuLU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "lkjx7nuzuO4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "JgJPuHiJul78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.isnull().sum())\n",
        "print(test_data.isnull().sum())"
      ],
      "metadata": {
        "id": "3TfCaeSrutJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.iloc[:, -1].unique()"
      ],
      "metadata": {
        "id": "iBLWXVKTu0hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.iloc[:, -1].value_counts().plot(kind='bar', title='Class Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kov8VDO2u5pM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Seperate features and labels"
      ],
      "metadata": {
        "id": "jb11WMldvRbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=train_data.iloc[:,:-1].values\n",
        "y_train=train_data.iloc[:,-1].values\n",
        "\n",
        "X_test=test_data.iloc[:,:-1].values\n",
        "y_test=test_data.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "xmn0hoQUvL7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply cross validation"
      ],
      "metadata": {
        "id": "B_PSqF2d0gj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "X =X_train\n",
        "y =y_train\n",
        "k_folds = 5\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    print(f'\\nFOLD {fold + 1}')\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]"
      ],
      "metadata": {
        "id": "qS3XHIW50mQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Random Heartbeats from the train_data"
      ],
      "metadata": {
        "id": "5C2tEj903rPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffle the train_data\n",
        "from sklearn.utils import shuffle\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "plt.figure(figsize=(10, 7))\n",
        "import random\n",
        "random_indices = random.sample(range(len(X_train)), 5)\n",
        "for i, idx in enumerate(random_indices):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.plot(X_train[idx])\n",
        "    plt.title(f\"Label: {int(y_train[idx])}\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "#plt.suptitle(\"Random Heartbeats from Training Data\", fontsize=16)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "roTuO9YS3hsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If y_train is a NumPy array, convert it to a pandas Series\n",
        "import pandas as pd\n",
        "y_train_series = pd.Series(y_train, name='Class')\n",
        "\n",
        "# Plot class distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='Class', data=y_train_series.to_frame())\n",
        "plt.title('Class Distribution Before Balancing')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1iUSfHVIBhYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( pd.Series(y_train).value_counts())"
      ],
      "metadata": {
        "id": "tTxeExKnDJUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balance the dataset by:\n",
        "\n",
        "#Downsampling class 0 to 10,000 samples\n",
        "\n",
        "#Upsampling classes 1 and 3 to 4,000 samples each\n",
        "\n",
        "#Keeping other classes (2 and 4) as they are"
      ],
      "metadata": {
        "id": "m2ShIQ-6C6xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# 1. Combine X and y\n",
        "df = pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name='label')], axis=1)\n",
        "\n",
        "# 2. Downsample class 0 to 10,000\n",
        "df_majority = df[df['label'] == 0.0]\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                   replace=False,\n",
        "                                   n_samples=10000,\n",
        "                                   random_state=42)\n",
        "\n",
        "# 3. Keep classes 2 and 4 as-is\n",
        "df_class_2 = df[df['label'] == 2.0]\n",
        "df_class_4 = df[df['label'] == 4.0]\n",
        "\n",
        "# 4. Collect classes 1 and 3 for upsampling\n",
        "df_class_1 = df[df['label'] == 1.0]\n",
        "df_class_3 = df[df['label'] == 3.0]\n",
        "\n",
        "# 5. Upsample class 1 to 4=4000\n",
        "df_class_1_upsampled = resample(df_class_1,\n",
        "                                replace=True,\n",
        "                                n_samples=4000,\n",
        "                                random_state=42)\n",
        "\n",
        "# 6. Upsample class 3 to 4000\n",
        "df_class_3_upsampled = resample(df_class_3,\n",
        "                                replace=True,\n",
        "                                n_samples=4000,\n",
        "                                random_state=42)\n",
        "\n",
        "# 7. Combine all classes\n",
        "df_balanced = pd.concat([\n",
        "    df_majority_downsampled,\n",
        "    df_class_2,\n",
        "    df_class_4,\n",
        "    df_class_1_upsampled,\n",
        "    df_class_3_upsampled\n",
        "]).sample(frac=1, random_state=42)  # shuffle\n",
        "\n",
        "# 8. Separate X and y\n",
        "X_train_balanced = df_balanced.drop('label', axis=1).values\n",
        "y_train_balanced = df_balanced['label'].values\n"
      ],
      "metadata": {
        "id": "TYzsbuEe4LfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( pd.Series(y_train_balanced).value_counts())"
      ],
      "metadata": {
        "id": "hXnZgMB03YV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x=y_train_balanced, palette='coolwarm')\n",
        "plt.title(\"Class Distribution After Balancing\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Sample Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AOQLjfop8S1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalize the ECG signals"
      ],
      "metadata": {
        "id": "1XMQP5b4v7s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train_balanced\n",
        "y_train=y_train_balanced\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "BN1glXbpxRJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Random Heartbeats from the train_data after balancing it"
      ],
      "metadata": {
        "id": "9qE9KdQ0GFCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "import random\n",
        "random_indices = random.sample(range(len(X_train)), 5)\n",
        "for i, idx in enumerate(random_indices):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.plot(X_train[idx])\n",
        "    plt.title(f\"Label: {int(y_train[idx])}\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.tight_layout()\n",
        "#plt.suptitle(\"Random Heartbeats from Training Data\", fontsize=16)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "aclIEGm6F-V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert to a PyTorch Dataset"
      ],
      "metadata": {
        "id": "QevGgc82Ev40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ECGDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "htSwFdSGEf8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ECGDataset(X_train, y_train)\n",
        "test_dataset = ECGDataset(X_test, y_test)\n",
        "val_dataset = ECGDataset(X_val, y_val)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "train_dataloader, val_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "Ca_LMiy2E2CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generate Wavelet Features(for Dual Branch)"
      ],
      "metadata": {
        "id": "OTKmBB9AFb_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets"
      ],
      "metadata": {
        "id": "pFTyHPNTGaOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pywt\n",
        "# --- Wavelet Feature Extraction ---\n",
        "def compute_wavelet(signal):\n",
        "    coeffs, _ = pywt.cwt(signal, scales=np.arange(1, 31), wavelet='morl')\n",
        "    return np.abs(coeffs).mean(axis=0)\n",
        "\n",
        "# Apply to all training and validation data\n",
        "wavelet_train = np.array([compute_wavelet(x) for x in X_train])\n",
        "wavelet_test = np.array([compute_wavelet(x) for x in X_test])\n",
        "wavelet_val = np.array([compute_wavelet(x) for x in X_val])"
      ],
      "metadata": {
        "id": "5ZcSpK3lFKW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualize Random Frequential (Wavelet) Signals from the wavelet_train"
      ],
      "metadata": {
        "id": "CmxjlCe8HmBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select 5 random indices\n",
        "random_indices = random.sample(range(len(wavelet_train)), 5)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, idx in enumerate(random_indices):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.plot(wavelet_train[idx])\n",
        "    plt.title(f\"Label: {int(y_train[idx])}\")\n",
        "    plt.xlabel(\"Freq\")\n",
        "    plt.ylabel(\"Magnitude\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Uncomment if you want a title over the whole figure\n",
        "# plt.suptitle(\"Random Frequential Representations (Wavelet)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2MQwZgO8Gq-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture (Dual-Branch Transformer)\n",
        "class ECGTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, dropout_rate=0.5):\n",
        "        super(ECGTransformer, self).__init__()\n",
        "\n",
        "        # Temporal branch\n",
        "        self.temporal_proj = nn.Linear(input_dim, 64)\n",
        "        self.temporal_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=64, nhead=8, dim_feedforward=256),\n",
        "            num_layers=3\n",
        "        )\n",
        "\n",
        "        # Frequency branch (assuming wavelet features)\n",
        "        self.freq_proj = nn.Linear(input_dim, 64)\n",
        "        self.freq_transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=64, nhead=8, dim_feedforward=256),\n",
        "            num_layers=3\n",
        "        )\n",
        "\n",
        "        # Combine branches\n",
        "        self.combine = nn.Linear(128, 64)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(64, num_classes)\n",
        "\n",
        "        # Batch normalization\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Temporal branch\n",
        "        x_temp = self.temporal_proj(x)\n",
        "        x_temp = self.bn1(x_temp.transpose(1, 2)).transpose(1, 2)\n",
        "        x_temp = self.temporal_transformer(x_temp)\n",
        "        x_temp = x_temp.mean(dim=1)\n",
        "\n",
        "        # Frequency branch (using same input for demo - replace with wavelet features)\n",
        "        x_freq = self.freq_proj(x)\n",
        "        x_freq = self.bn2(x_freq.transpose(1, 2)).transpose(1, 2)\n",
        "        x_freq = self.freq_transformer(x_freq)\n",
        "        x_freq = x_freq.mean(dim=1)\n",
        "\n",
        "        # Combine branches\n",
        "        x = torch.cat([x_temp, x_freq], dim=1)\n",
        "        x = self.combine(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = ECGTransformer(input_dim=X_train.shape[1], num_classes=5).to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "O_e641uIHzzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = np.bincount(y_train.astype(int))\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# Loss function with class weights\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer with weight decay (L2 regularization)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n"
      ],
      "metadata": {
        "id": "6ECdBNF2JzkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "accumulation_steps = 2\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "epochs=3\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for i,(inputs, labels) in enumerate(train_dataloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "    with torch.cuda.amp.autocast():\n",
        "      outputs = model(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)/accumulation_steps\n",
        "\n",
        "    # Backward pass and optimizer\n",
        "    loss.backward()\n",
        "    if (i + 1) % accumulation_steps == 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      val_total += labels.size(0)\n",
        "      val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "      # Calculate metrics\n",
        "  train_loss = running_loss / len(train_dataloader)\n",
        "  val_loss = val_loss / len(val_dataloader)\n",
        "  train_acc = 100. * correct / total\n",
        "  val_acc = 100. * val_correct / val_total\n",
        "\n",
        "  # Update learning rate\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "        # Store metrics\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  val_accs.append(val_acc)\n",
        "\n",
        "  print(f'Epoch: {epoch+1} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.2f}% | '\n",
        "              f'Val Loss: {val_loss:.5f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), 'best_model.pth')\n",
        "    patience_counter = 0\n",
        "  else:\n",
        "    patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "      print(\"Early stopping triggered\")\n",
        "      break"
      ],
      "metadata": {
        "id": "D38I-of2Lfda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "accumulation_steps = 2\n",
        "train_losses, val_losses ,test_losses= [], [],[]\n",
        "train_accs, val_accs,test_accs = [], [],[]\n",
        "epochs=100\n",
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for i,(inputs, labels) in enumerate(train_dataloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "    with torch.cuda.amp.autocast():\n",
        "      outputs = model(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)/accumulation_steps\n",
        "\n",
        "    # Backward pass and optimizer\n",
        "    loss.backward()\n",
        "    if (i + 1) % accumulation_steps == 0:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  val_correct = 0\n",
        "  val_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in val_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      val_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      val_total += labels.size(0)\n",
        "      val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Testing\n",
        "  model.eval()\n",
        "  test_loss = 0.0\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = model(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      test_total += labels.size(0)\n",
        "      test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "\n",
        "      # Calculate metrics\n",
        "  train_loss = running_loss / len(train_dataloader)\n",
        "  val_loss = val_loss / len(val_dataloader)\n",
        "  test_loss=test_loss/len(test_dataloader)\n",
        "  train_acc = 100. * correct / total\n",
        "  val_acc = 100. * val_correct / val_total\n",
        "  test_acc=100.*test_correct/test_total\n",
        "\n",
        "  # Update learning rate\n",
        "  scheduler.step(val_loss)\n",
        "\n",
        "        # Store metrics\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "  test_losses.append(test_loss)\n",
        "  train_accs.append(train_acc)\n",
        "  val_accs.append(val_acc)\n",
        "  test_accs.append(test_acc)\n",
        "  if (epoch + 1) == 1 or (epoch + 1) % 10 == 0:\n",
        "    print(f'Epoch: {epoch+1} | Train Loss: {train_loss:.5f}, Train Acc: {train_acc:.2f}% | '\n",
        "              f'Val Loss: {val_loss:.5f}, Val Acc: {val_acc:.2f}% |'\n",
        "              f'Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%')\n",
        "\n",
        "  # still save the best model\n",
        "  if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    torch.save(model.state_dict(), 'best_model.pth')\n"
      ],
      "metadata": {
        "id": "59GEMLwaOeco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_range = range(1, len(train_losses) + 1)\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Left subplot: Train Loss & Train Accuracy\n",
        "axs[0].plot(epochs_range, train_losses, label='Train Loss', color='tab:blue')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss', color='tab:blue')\n",
        "axs[0].tick_params(axis='y', labelcolor='tab:blue')\n",
        "axs[0].set_title('Train Loss and Accuracy')\n",
        "\n",
        "ax2 = axs[0].twinx()\n",
        "ax2.plot(epochs_range, train_accs, label='Train Accuracy', color='tab:orange')\n",
        "ax2.set_ylabel('Accuracy (%)', color='tab:orange')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
        "\n",
        "# Right subplot: Test Loss & Test Accuracy\n",
        "axs[1].plot(epochs_range, test_losses, label='Test Loss', color='tab:blue')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Loss', color='tab:blue')\n",
        "axs[1].tick_params(axis='y', labelcolor='tab:blue')\n",
        "axs[1].set_title('Test Loss and Accuracy')\n",
        "\n",
        "ax4 = axs[1].twinx()\n",
        "ax4.plot(epochs_range, test_accs, label='Test Accuracy', color='tab:orange')\n",
        "ax4.set_ylabel('Accuracy (%)', color='tab:orange')\n",
        "ax4.tick_params(axis='y', labelcolor='tab:orange')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EZrA2V_VSvMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_results = pd.DataFrame([{\n",
        "    'model_name': model.__class__.__name__,\n",
        "    'model_loss': test_loss,\n",
        "    'model_acc': test_acc\n",
        "}])\n",
        "\n",
        "model_results"
      ],
      "metadata": {
        "id": "F-kX0lAWTDpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Make and evaluate random prediction with our model"
      ],
      "metadata": {
        "id": "OmcKRbMFmW-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n",
        "    pred_probs = []\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for sample in data:\n",
        "            # Prepare sample\n",
        "            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n",
        "            # Reshape the sample to have 3 dimensions for the model input (unsqueeze on dim=1)\n",
        "            sample = sample.unsqueeze(1)\n",
        "            # Forward pass (model outputs raw logit)\n",
        "            pred_logit = model(sample)\n",
        "\n",
        "            # Get prediction probability (logit -> prediction probability)\n",
        "            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
        "\n",
        "            # Get pred_prob off GPU for further calculations\n",
        "            pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "    # Stack the pred_probs to turn list into a tensor\n",
        "    return torch.stack(pred_probs)"
      ],
      "metadata": {
        "id": "yEMi_kIxTw9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "# Get a list of random row indices\n",
        "random_indices = random.sample(range(len(test_data)), k=9)\n",
        "\n",
        "# Use the indices to access data and labels\n",
        "for idx in random_indices:\n",
        "    # Convert sample to PyTorch tensor\n",
        "    sample = torch.tensor(test_data.iloc[idx, :-1].values, dtype=torch.float32)\n",
        "    label = test_data.iloc[idx, -1]\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "\n",
        "# View the first test sample shape and label\n",
        "print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]}\")"
      ],
      "metadata": {
        "id": "tIAWdxz2meJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test samples with model 2\n",
        "pred_probs= make_predictions(model=model,\n",
        "                             data=test_samples)\n",
        "\n",
        "# View first 5 prediction probabilities list\n",
        "pred_probs[:5]"
      ],
      "metadata": {
        "id": "oUYctxidmhR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the prediction probabilities into prediction labels by taking the argmax()\n",
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ],
      "metadata": {
        "id": "PIWhh1SNmjhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Are our predictions in the same form as our test labels?\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "print(f'test labels :{test_labels_tensor}')\n",
        "print(f'pred calsses:{pred_classes}')"
      ],
      "metadata": {
        "id": "5CPBtdQImnAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's visualize:\n",
        "# Plot predictions\n",
        "plt.figure(figsize=(9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create a subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  plt.plot(sample.squeeze())\n",
        "\n",
        "  # Find the prediction label (in text form, e.g. \"0\")\n",
        "  pred_label = pred_classes[i]\n",
        "\n",
        "  # Get the truth label (in text form, e.g. \"2\")\n",
        "  truth_label = test_labels[i]\n",
        "\n",
        "  # Create the title text of the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # Check for equality and change title colour accordingly\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "vh9koyS9mqEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Making a confusion matrix for further prediction evaluation :!"
      ],
      "metadata": {
        "id": "RCC8COtinBuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "y_preds=[]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for input, label in tqdm(test_dataloader,desc='Making predictions'):\n",
        "    input, label = input.to(device), label.to(device)\n",
        "    y_pred = model(input.unsqueeze(1))\n",
        "    #Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)"
      ],
      "metadata": {
        "id": "Lmc4p0QxmsxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
        "except:\n",
        "    !pip install -q torchmetrics -U mlxtend\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")"
      ],
      "metadata": {
        "id": "Fh76mQ4fnEwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import mlxtend upgraded version\n",
        "import mlxtend\n",
        "print(mlxtend.__version__)\n",
        "assert int(mlxtend.__version__.split(\".\")[1]) >= 19"
      ],
      "metadata": {
        "id": "UtjP6VVDnJVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# 'y_test' contains the target labels\n",
        "confmat = ConfusionMatrix(num_classes=5, task='multiclass')\n",
        "confmat_tensor = confmat(preds=y_pred_tensor.argmax(dim=1),\n",
        "                         target=torch.tensor(y_test, dtype=torch.int64))  # Convert y_test to a PyTorch tensor\n",
        "\n",
        "# Define class names\n",
        "class_names = ['0', '1', '2', '3', '4']\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(),\n",
        "    class_names=class_names,\n",
        "    figsize=(10, 7)\n",
        ");"
      ],
      "metadata": {
        "id": "a69SHwPjnn1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import torch\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    Displays the confusion matrix with or without normalization.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "y_pred_labels = y_pred_tensor.argmax(dim=1).cpu().numpy()\n",
        "y_true_labels = torch.tensor(y_test).cpu().numpy()\n",
        "\n",
        "# --- Compute the confusion matrix using scikit-learn ---\n",
        "cnf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plot_confusion_matrix(cnf_matrix,\n",
        "                      classes= ['0', '1', '2', '3', '4'],\n",
        "                      normalize=True,\n",
        "                      title='Normalized Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yl9LCacPnr_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_labels, y_pred_labels, digits=4))"
      ],
      "metadata": {
        "id": "OZcRn0fLpqd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC and AUC"
      ],
      "metadata": {
        "id": "Dv2ydBRhrrp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import torch # Make sure torch is imported if not already\n",
        "\n",
        "n_classes = 5\n",
        "\n",
        "# Use y_true_labels which contains the ground truth labels\n",
        "y_true_bin = label_binarize(y_true_labels, classes=[0,1,2,3,4])\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "# Calculate the softmax probabilities to get class scores\n",
        "y_score = torch.softmax(y_pred_tensor, dim=1).cpu().numpy()\n",
        "\n",
        "for i in range(n_classes):\n",
        "    # Use the binarized true labels and the predicted scores\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
        "             label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)  # diagonal line\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multi-class ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cpNwhcTAp2SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# y_true: true labels (shape: [n_samples])\n",
        "# y_score: predicted probabilities (shape: [n_samples, n_classes])\n",
        "# Binarize true labels for one-vs-rest approach\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "n_classes = y_score.shape[1]\n",
        "# Corrected: Use y_true_labels instead of y_true\n",
        "y_test_bin = label_binarize(y_true_labels, classes=np.arange(n_classes))\n",
        "\n",
        "# Plot precision-recall curve for each class\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(n_classes):\n",
        "    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])\n",
        "    avg_prec = average_precision_score(y_test_bin[:, i], y_score[:, i])\n",
        "    plt.plot(recall, precision, lw=2, label=f'Class {i} (AP = {avg_prec:.2f})')\n",
        "\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Multi-class Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZmdFr7Z9qfJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save and load our model"
      ],
      "metadata": {
        "id": "tEZ_GBUmsuvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True,exist_ok=True )\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"ECG_arrhythmia_classification_model_3.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "0Ouhsx6mrv6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Now we've got a saved model state_dict() we can load it back in using a combination of load_state_dict() and torch.load().\n",
        "\n",
        "#####Since we're using load_state_dict(), we'll need to create a new instance of ECGTransformer() with the same input parameters as our saved model state_dict()"
      ],
      "metadata": {
        "id": "LwGFrFmcs4bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n",
        "#Provide input_dim and num_classes while creating an instance of ECGTransformerV3\n",
        "loaded_model = ECGTransformer(input_dim=X_train.shape[1],num_classes=5)\n",
        "\n",
        "# Load in the saved state_dict()\n",
        "loaded_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# Send model to GPU\n",
        "loaded_model= loaded_model.to(device)"
      ],
      "metadata": {
        "id": "yK3RVTwls0_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate through the keys and compare individual tensors using torch.equal()\n",
        "all_equal = True\n",
        "for key in loaded_model.state_dict():\n",
        "    if not torch.equal(loaded_model.state_dict()[key], loaded_model.state_dict()[key]):\n",
        "        all_equal = False\n",
        "        break\n",
        "\n",
        "print(all_equal)  # This will print True if all tensors are equal"
      ],
      "metadata": {
        "id": "uXknAqSSs-DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Now that we've got a loaded model we can evaluate it to make sure its parameters work similarly to model_3 prior to saving"
      ],
      "metadata": {
        "id": "H3hS6JOMtNBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  loaded_model.eval()\n",
        "  loaded_test_loss = 0.0\n",
        "  test_correct = 0\n",
        "  test_total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in test_dataloader:\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      outputs = loaded_model(inputs.unsqueeze(1))\n",
        "      loss = loss_fn(outputs, labels)\n",
        "\n",
        "      loaded_test_loss += loss.item()\n",
        "      _, predicted = outputs.max(1)\n",
        "      test_total += labels.size(0)\n",
        "      test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "\n",
        "      # Calculate metrics\n",
        "  loaded_test_loss=loaded_test_loss/len(test_dataloader)\n",
        "  loaded_test_acc=100.*test_correct/test_total\n",
        "\n",
        "  # Update learning rate\n",
        "  scheduler.step(test_loss)\n"
      ],
      "metadata": {
        "id": "CpNKhpdutJaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_results = pd.DataFrame([{\n",
        "    'model_name': loaded_model.__class__.__name__,\n",
        "    'model_loss': loaded_test_loss,\n",
        "    'model_acc': loaded_test_acc\n",
        "}])\n",
        "loaded_model_results"
      ],
      "metadata": {
        "id": "CAS35TYotS19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_results"
      ],
      "metadata": {
        "id": "p_494n8ftuF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8kKPbhDtwdM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}